import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import torch
import torch.nn as nn
import torch.optim as optim
import scipy.stats as stats
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt



try:
  # Wczytanie danych
  housing = fetch_california_housing()
  # Przekształcenie obiektu Bunch na DataFrame
  data = pd.DataFrame(housing.data, columns=housing.feature_names)
  X = data.values  
  y = housing.target  
except Exception as e:
  print(f"Nie udało się wczytać danych: {e}")
  X = None
  y = None

 # blokada pustych danychpip
if X is not None and y is not None:
    # Analiza danych
    print("Analiza danych:")
    sns.pairplot(data)
    plt.show()
    # Obliczenie procentu outlierów
    outliers = np.abs(stats.zscore(X))
    outlier_percentage = np.mean(outliers > 3) * 100
    print(f"Procent outlierów: {outlier_percentage}%")

    # Przygotowanie danych
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Podział danych na treningowe i testowe
    train_size = 0.75
    test_size = 0.25
    X_train, X_test, y_train, y_test = train_test_split(X_scaled,
                                                        y,
                                                        test_size=test_size,
                                                        random_state=42)

    # Normalizacja danych testowych
    X_test_scaled = scaler.transform(X_test)
    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).view(-1, 1, 8)

    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_train = torch.tensor(X_train, dtype=torch.float32).view(-1, 1, 8)



    # Konfiguracja modelu
    class ConvNet(nn.Module):
        def __init__(self):
            super(ConvNet, self).__init__()
            self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)
            self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
            self.fc1 = nn.Linear(16 * 5, 1)

        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = x.view(-1, 16 * 5)
            x = F.relu(self.fc1(x))
            return x

    model = ConvNet()

    # Funkcja straty i optymalizator
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Trenowanie modelu
    num_epochs = 100
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(torch.tensor(X_train, dtype=torch.float32))
        loss = criterion(outputs, torch.tensor(y_train, dtype=torch.float32))
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

    # Przewidywanie na danych testowych
    model.eval()
    with torch.no_grad():
        X_test = X_test.reshape(-1, 1, 8)
        predictions = model(torch.tensor(X_test, dtype=torch.float32))

    # Obliczenie metryk
    y_test = y_test[:predictions.shape[0]]
    mse = mean_squared_error(y_test, predictions.numpy())
    rmse = sqrt(mse)
    mae = mean_absolute_error(y_test, predictions.numpy())

    print(f"RMSE: {rmse}, MAE: {mae}")

    # Wyświetlenie wyników
    plt.scatter(y_test, predictions.numpy(), alpha=0.5)
    plt.xlabel('Oryginalne ceny')
    plt.ylabel('Przewidziane ceny')
    plt.title('Wykres przewidzianych vs oryginalnych cen')
    plt.show()
else:
    print("Nie udało się kontynuować z powodu problemów z wczytaniem danych.")
